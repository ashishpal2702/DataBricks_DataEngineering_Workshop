{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4761dd-22bb-4850-8f17-b4700a5b5364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Structured Streaming - Complete Tutorial\n",
    "\n",
    "## \uD83D\uDCDA Learning Path: Basic to Advanced\n",
    "\n",
    "This comprehensive tutorial covers Spark Structured Streaming from fundamental concepts to advanced production patterns:\n",
    "\n",
    "### \uD83C\uDFAF **Tutorial Outline**\n",
    "\n",
    "#### **\uD83D\uDCD6 Part I: Fundamentals (Basic)**\n",
    "1. **Environment Setup** - Spark configuration and initialization\n",
    "2. **Data Sources & Schema** - Input sources and data modeling\n",
    "3. **ReadStream Operations** - Reading from various sources\n",
    "4. **Basic Transformations** - Filtering, selecting, and simple operations\n",
    "\n",
    "#### **⚙️ Part II: Core Concepts (Intermediate)**\n",
    "5. **Triggers & Processing** - Controlling micro-batch execution\n",
    "6. **Output Modes** - Append, Complete, Update strategies\n",
    "7. **WriteStream Operations** - Various output sinks\n",
    "8. **Basic Aggregations** - Count, sum, average operations\n",
    "\n",
    "#### **\uD83D\uDD25 Part III: Advanced Concepts (Advanced)**\n",
    "9. **State Management** - Stateful vs Stateless operations\n",
    "10. **Window Operations** - Tumbling, Sliding, Session windows\n",
    "11. **Watermarks** - Late data handling and state cleanup\n",
    "12. **Streaming Joins** - Stream-to-stream and stream-to-static joins\n",
    "13. **Advanced Aggregations** - Approximate functions and complex analytics\n",
    "14. **RocksDB State Store** - Production state management\n",
    "\n",
    "#### **\uD83C\uDFED Part IV: Production Patterns (Expert)**\n",
    "15. **Kafka Integration** - Real-world streaming with Kafka\n",
    "16. **Monitoring & Debugging** - Query monitoring and troubleshooting\n",
    "17. **Performance Optimization** - Best practices and tuning\n",
    "18. **Complete Pipeline** - End-to-end production example\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83C\uDF93 Prerequisites\n",
    "- Basic knowledge of Apache Spark and PySpark\n",
    "- Understanding of SQL and DataFrames\n",
    "- Familiarity with streaming concepts\n",
    "\n",
    "## \uD83D\uDEE0️ What You'll Build\n",
    "By the end of this tutorial, you'll have built:\n",
    "- Real-time analytics dashboard\n",
    "- Customer behavior tracking system\n",
    "- Fraud detection pipeline\n",
    "- Complete Kafka streaming application\n",
    "\n",
    "Let's begin! \uD83D\uDE80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884de613-7f28-4157-bbcb-4c136a0e077d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDCD6 Part I: Fundamentals\n",
    "\n",
    "## 1. Environment Setup and Initialization\n",
    "\n",
    "First, let's set up our Spark environment with the necessary configurations for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fbb6cb-6215-4d51-aca9-78fc150b7a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Spark Structured Streaming Environment Ready!\nSpark Version: 4.0.0\n✅ Configured with RocksDB state store for production workloads\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create Spark Session with streaming configurations\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"StructuredStreamingCompleteTutorial\") \\\n",
    "#     .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming_checkpoints\") \\\n",
    "#     .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#     .config(\"spark.sql.streaming.stateStore.providerClass\", \n",
    "#             \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\") \\\n",
    "#     .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "#spark.sparkContext.setLogLevel(\"WARN\")\n",
    "data_dir = \"/Volumes/workspace/default/stream/spark_streaming_workshop\"\n",
    "\n",
    "print(\"\uD83D\uDE80 Spark Structured Streaming Environment Ready!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "#print(f\"Session ID: {spark.sparkContext.applicationId}\")\n",
    "print(\"✅ Configured with RocksDB state store for production workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7794ef49-9400-4535-b2d7-8d89d35dcc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Data Sources and Schema Definition\n",
    "\n",
    "Understanding data schemas and sources is fundamental to streaming applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e023c6a-ab74-4f6d-9f5d-61ad376841ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Streaming Schema Defined:\n  • customer_id: LongType() (nullable: True)\n  • order_id: LongType() (nullable: True)\n  • product_name: StringType() (nullable: True)\n  • quantity: IntegerType() (nullable: True)\n  • price: DoubleType() (nullable: True)\n  • order_timestamp: TimestampType() (nullable: True)\n  • category: StringType() (nullable: True)\n  • customer_location: StringType() (nullable: True)\n  • payment_method: StringType() (nullable: True)\n\n\uD83D\uDCCB Sample Data Generated:\n\n\uD83D\uDD38 Record 1:\n{\n  \"customer_id\": 8083,\n  \"order_id\": 186772,\n  \"product_name\": \"MacBook Pro\",\n  \"quantity\": 3,\n  \"price\": 2101.79,\n  \"order_timestamp\": \"2025-09-14T16:12:21.685199\",\n  \"category\": \"Electronics\",\n  \"customer_location\": \"California\",\n  \"payment_method\": \"paypal\"\n}\n\n\uD83D\uDD38 Record 2:\n{\n  \"customer_id\": 2417,\n  \"order_id\": 324723,\n  \"product_name\": \"Wireless Headphones\",\n  \"quantity\": 5,\n  \"price\": 231.04,\n  \"order_timestamp\": \"2025-09-14T16:06:15.685199\",\n  \"category\": \"Electronics\",\n  \"customer_location\": \"Texas\",\n  \"payment_method\": \"debit_card\"\n}\n\n\uD83D\uDD38 Record 3:\n{\n  \"customer_id\": 1295,\n  \"order_id\": 780026,\n  \"product_name\": \"Smart Watch\",\n  \"quantity\": 3,\n  \"price\": 434.71,\n  \"order_timestamp\": \"2025-09-14T16:09:35.685199\",\n  \"category\": \"Electronics\",\n  \"customer_location\": \"California\",\n  \"payment_method\": \"paypal\"\n}\n\n\uD83D\uDD38 Record 4:\n{\n  \"customer_id\": 6977,\n  \"order_id\": 477436,\n  \"product_name\": \"MacBook Pro\",\n  \"quantity\": 1,\n  \"price\": 2243.06,\n  \"order_timestamp\": \"2025-09-14T16:49:22.685199\",\n  \"category\": \"Electronics\",\n  \"customer_location\": \"Florida\",\n  \"payment_method\": \"credit_card\"\n}\n\n\uD83D\uDD38 Record 5:\n{\n  \"customer_id\": 9768,\n  \"order_id\": 307022,\n  \"product_name\": \"Coffee Maker\",\n  \"quantity\": 4,\n  \"price\": 106.93,\n  \"order_timestamp\": \"2025-09-14T16:20:17.685199\",\n  \"category\": \"Home\",\n  \"customer_location\": \"California\",\n  \"payment_method\": \"apple_pay\"\n}\n\n✅ Generated 5 sample records\n"
     ]
    }
   ],
   "source": [
    "# Sample data generator for streaming\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import builtins  # <-- Add this import to use Python's built-in round\n",
    "# Define comprehensive schema for our streaming data\n",
    "streaming_schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", TimestampType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"customer_location\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"\uD83D\uDCCA Streaming Schema Defined:\")\n",
    "for field in streaming_schema.fields:\n",
    "    print(f\"  • {field.name}: {field.dataType} (nullable: {field.nullable})\")\n",
    "\n",
    "# Sample data generator for realistic streaming simulation\n",
    "def generate_sample_data(num_records=10):\n",
    "    \"\"\"Generate realistic e-commerce sample data\"\"\"\n",
    "    \n",
    "    products = [\n",
    "        {\"name\": \"iPhone 15\", \"category\": \"Electronics\", \"base_price\": 999.99},\n",
    "        {\"name\": \"MacBook Pro\", \"category\": \"Electronics\", \"base_price\": 1999.99},\n",
    "        {\"name\": \"Nike Air Max\", \"category\": \"Fashion\", \"base_price\": 129.99},\n",
    "        {\"name\": \"Coffee Maker\", \"category\": \"Home\", \"base_price\": 89.99},\n",
    "        {\"name\": \"Gaming Chair\", \"category\": \"Furniture\", \"base_price\": 299.99},\n",
    "        {\"name\": \"Wireless Headphones\", \"category\": \"Electronics\", \"base_price\": 199.99},\n",
    "        {\"name\": \"Yoga Mat\", \"category\": \"Sports\", \"base_price\": 49.99},\n",
    "        {\"name\": \"Smart Watch\", \"category\": \"Electronics\", \"base_price\": 399.99}\n",
    "    ]\n",
    "    \n",
    "    locations = [\"New York\", \"California\", \"Texas\", \"Florida\", \"Illinois\"]\n",
    "    payment_methods = [\"credit_card\", \"debit_card\", \"paypal\", \"apple_pay\"]\n",
    "    \n",
    "    sample_data = []\n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        product = random.choice(products)\n",
    "        customer_id = random.randint(1001, 9999)\n",
    "        order_id = random.randint(100000, 999999)\n",
    "        quantity = random.randint(1, 5)\n",
    "        price_variation = random.uniform(0.8, 1.2)\n",
    "        price = builtins.round(product[\"base_price\"] * price_variation, 2)\n",
    "        time_offset = random.randint(0, 3600)  # Last hour\n",
    "        order_timestamp = base_time - timedelta(seconds=time_offset)\n",
    "        \n",
    "        record = {\n",
    "            \"customer_id\": customer_id,\n",
    "            \"order_id\": order_id,\n",
    "            \"product_name\": product[\"name\"],\n",
    "            \"quantity\": quantity,\n",
    "            \"price\": price,\n",
    "            \"order_timestamp\": order_timestamp.isoformat(),\n",
    "            \"category\": product[\"category\"],\n",
    "            \"customer_location\": random.choice(locations),\n",
    "            \"payment_method\": random.choice(payment_methods)\n",
    "        }\n",
    "        sample_data.append(record)\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Generate and display sample data\n",
    "sample_records = generate_sample_data(5)\n",
    "print(\"\\n\uD83D\uDCCB Sample Data Generated:\")\n",
    "for i, record in enumerate(sample_records, 1):\n",
    "    print(f\"\\n\uD83D\uDD38 Record {i}:\")\n",
    "    print(json.dumps(record, indent=2, default=str))\n",
    "\n",
    "print(f\"\\n✅ Generated {len(sample_records)} sample records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf025a22-777a-4434-8c54-a1184bce581d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. ReadStream Operations - Data Sources\n",
    "\n",
    "Structured Streaming supports various input sources. Let's explore the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374ad3d0-2fa0-45e9-854c-4313f108e3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Sample files created in: /Volumes/workspace/default/stream/spark_streaming_workshop/json_input\n"
     ]
    }
   ],
   "source": [
    "# Create sample files for file-based streaming\n",
    "def create_sample_files(output_dir=\"/tmp/streaming_input\", num_files=3):\n",
    "    \"\"\"Create sample JSON files for file-based streaming\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file_num in range(1, num_files + 1):\n",
    "        file_data = generate_sample_data(5)\n",
    "        filename = f\"orders_batch_{file_num:03d}.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            for record in file_data:\n",
    "                f.write(json.dumps(record, default=str) + '\\n')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Create sample files\n",
    "output_dir = f\"{data_dir}/json_input\"\n",
    "file_input_path = create_sample_files(output_dir)\n",
    "print(f\"\uD83D\uDCC1 Sample files created in: {file_input_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332985ae-9c5a-4992-938d-a6d892ef755e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e7c5619-1787-48b4-befe-ca46e1c9bce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Basic Transformations\n",
    "\n",
    "Learn fundamental streaming transformations that don't require state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad3ad25-3c2a-4a86-b3aa-1f280dd1d0ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File stream configured\n\n\uD83D\uDCCA Available Streaming Sources:\n  • rate_stream - Basic rate source\n  • enhanced_rate_stream - Realistic e-commerce data\n  • file_stream - JSON file monitoring\n"
     ]
    }
   ],
   "source": [
    "# File stream configuration\n",
    "file_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(streaming_schema) \\\n",
    "    .option(\"path\", file_input_path) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "\n",
    "print(\"✅ File stream configured\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Available Streaming Sources:\")\n",
    "print(\"  • rate_stream - Basic rate source\")\n",
    "print(\"  • enhanced_rate_stream - Realistic e-commerce data\")\n",
    "print(\"  • file_stream - JSON file monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca9e971-09d0-4c72-8bff-d08a144f7c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: long (nullable = true)\n |-- order_id: long (nullable = true)\n |-- product_name: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- order_timestamp: timestamp (nullable = true)\n |-- category: string (nullable = true)\n |-- customer_location: string (nullable = true)\n |-- payment_method: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "file_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32063994-9761-4515-a3f3-2ad44b71b59d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_name</th><th>price</th><th>category</th><th>order_timestamp</th></tr></thead><tbody><tr><td>6349</td><td>Yoga Mat</td><td>46.31</td><td>Sports</td><td>2025-09-14T16:43:49.516Z</td></tr><tr><td>7198</td><td>Coffee Maker</td><td>76.4</td><td>Home</td><td>2025-09-14T16:17:52.516Z</td></tr><tr><td>8588</td><td>Wireless Headphones</td><td>208.24</td><td>Electronics</td><td>2025-09-14T16:58:33.516Z</td></tr><tr><td>2851</td><td>Coffee Maker</td><td>87.45</td><td>Home</td><td>2025-09-14T16:12:32.516Z</td></tr><tr><td>3385</td><td>Gaming Chair</td><td>288.07</td><td>Furniture</td><td>2025-09-14T16:48:16.516Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6349,
         "Yoga Mat",
         46.31,
         "Sports",
         "2025-09-14T16:43:49.516Z"
        ],
        [
         7198,
         "Coffee Maker",
         76.4,
         "Home",
         "2025-09-14T16:17:52.516Z"
        ],
        [
         8588,
         "Wireless Headphones",
         208.24,
         "Electronics",
         "2025-09-14T16:58:33.516Z"
        ],
        [
         2851,
         "Coffee Maker",
         87.45,
         "Home",
         "2025-09-14T16:12:32.516Z"
        ],
        [
         3385,
         "Gaming Chair",
         288.07,
         "Furniture",
         "2025-09-14T16:48:16.516Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_timestamp",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.1 Stateless Transformations (No state between batches)\n",
    "print(\"\uD83D\uDD04 Basic Streaming Transformations\")\n",
    "print(\"\\n\uD83D\uDCDD Stateless Operations (Process each micro-batch independently):\")\n",
    "\n",
    "# Filtering and selection\n",
    "filtered_stream = file_stream \\\n",
    "    .filter(col(\"price\") > 10) \\\n",
    "    .select(\"customer_id\", \"product_name\", \"price\", \"category\", \"order_timestamp\")\n",
    "\n",
    "print(\"✅ Filtering: Orders > $10\")\n",
    "display(filtered_stream.limit(5), \n",
    "        checkpointLocation=f\"{data_dir}/filtered_stream_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8a0656-d35e-4911-83ba-25da645a1571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>order_id</th><th>product_name</th><th>quantity</th><th>price</th><th>order_timestamp</th><th>category</th><th>customer_location</th><th>payment_method</th><th>total_amount</th><th>order_value_tier</th><th>is_high_quantity</th><th>processing_time</th></tr></thead><tbody><tr><td>6349</td><td>547863</td><td>Yoga Mat</td><td>4</td><td>46.31</td><td>2025-09-14T16:43:49.516Z</td><td>Sports</td><td>New York</td><td>apple_pay</td><td>185.24</td><td>Basic</td><td>true</td><td>2025-09-14T17:13:02.830Z</td></tr><tr><td>7198</td><td>370693</td><td>Coffee Maker</td><td>3</td><td>76.4</td><td>2025-09-14T16:17:52.516Z</td><td>Home</td><td>Illinois</td><td>credit_card</td><td>229.20000000000002</td><td>Basic</td><td>true</td><td>2025-09-14T17:13:02.830Z</td></tr><tr><td>8588</td><td>604014</td><td>Wireless Headphones</td><td>4</td><td>208.24</td><td>2025-09-14T16:58:33.516Z</td><td>Electronics</td><td>California</td><td>credit_card</td><td>832.96</td><td>Standard</td><td>true</td><td>2025-09-14T17:13:02.830Z</td></tr><tr><td>2851</td><td>991716</td><td>Coffee Maker</td><td>2</td><td>87.45</td><td>2025-09-14T16:12:32.516Z</td><td>Home</td><td>California</td><td>credit_card</td><td>174.9</td><td>Basic</td><td>false</td><td>2025-09-14T17:13:02.830Z</td></tr><tr><td>3385</td><td>927432</td><td>Gaming Chair</td><td>3</td><td>288.07</td><td>2025-09-14T16:48:16.516Z</td><td>Furniture</td><td>New York</td><td>apple_pay</td><td>864.21</td><td>Standard</td><td>true</td><td>2025-09-14T17:13:02.830Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6349,
         547863,
         "Yoga Mat",
         4,
         46.31,
         "2025-09-14T16:43:49.516Z",
         "Sports",
         "New York",
         "apple_pay",
         185.24,
         "Basic",
         true,
         "2025-09-14T17:13:02.830Z"
        ],
        [
         7198,
         370693,
         "Coffee Maker",
         3,
         76.4,
         "2025-09-14T16:17:52.516Z",
         "Home",
         "Illinois",
         "credit_card",
         229.20000000000002,
         "Basic",
         true,
         "2025-09-14T17:13:02.830Z"
        ],
        [
         8588,
         604014,
         "Wireless Headphones",
         4,
         208.24,
         "2025-09-14T16:58:33.516Z",
         "Electronics",
         "California",
         "credit_card",
         832.96,
         "Standard",
         true,
         "2025-09-14T17:13:02.830Z"
        ],
        [
         2851,
         991716,
         "Coffee Maker",
         2,
         87.45,
         "2025-09-14T16:12:32.516Z",
         "Home",
         "California",
         "credit_card",
         174.9,
         "Basic",
         false,
         "2025-09-14T17:13:02.830Z"
        ],
        [
         3385,
         927432,
         "Gaming Chair",
         3,
         288.07,
         "2025-09-14T16:48:16.516Z",
         "Furniture",
         "New York",
         "apple_pay",
         864.21,
         "Standard",
         true,
         "2025-09-14T17:13:02.830Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "order_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "order_value_tier",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_high_quantity",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "processing_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding computed columns\n",
    "enriched_stream = file_stream \\\n",
    "    .withColumn(\"total_amount\", col(\"price\") * col(\"quantity\")) \\\n",
    "    .withColumn(\"order_value_tier\",\n",
    "        when(col(\"price\") > 500, \"Premium\")\n",
    "        .when(col(\"price\") > 200, \"Standard\")\n",
    "        .otherwise(\"Basic\")\n",
    "    ) \\\n",
    "    .withColumn(\"is_high_quantity\", col(\"quantity\") > 2) \\\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "print(\"✅ Column enrichment: Added total_amount, order_value_tier, processing_time\")\n",
    "display(enriched_stream.limit(5),checkpointLocation=f\"{data_dir}/enriched_stream_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bfbb5e-c399-4e56-aab8-b84b20b91a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>order_id</th><th>product_name</th><th>quantity</th><th>price</th><th>order_timestamp</th><th>category</th><th>customer_location</th><th>payment_method</th><th>product_upper</th><th>category_lower</th><th>location_formatted</th></tr></thead><tbody><tr><td>6349</td><td>547863</td><td>Yoga Mat</td><td>4</td><td>46.31</td><td>2025-09-14T16:43:49.516Z</td><td>Sports</td><td>New York</td><td>apple_pay</td><td>YOGA MAT</td><td>sports</td><td>City: New York</td></tr><tr><td>7198</td><td>370693</td><td>Coffee Maker</td><td>3</td><td>76.4</td><td>2025-09-14T16:17:52.516Z</td><td>Home</td><td>Illinois</td><td>credit_card</td><td>COFFEE MAKER</td><td>home</td><td>City: Illinois</td></tr><tr><td>8588</td><td>604014</td><td>Wireless Headphones</td><td>4</td><td>208.24</td><td>2025-09-14T16:58:33.516Z</td><td>Electronics</td><td>California</td><td>credit_card</td><td>WIRELESS HEADPHONES</td><td>electronics</td><td>City: California</td></tr><tr><td>2851</td><td>991716</td><td>Coffee Maker</td><td>2</td><td>87.45</td><td>2025-09-14T16:12:32.516Z</td><td>Home</td><td>California</td><td>credit_card</td><td>COFFEE MAKER</td><td>home</td><td>City: California</td></tr><tr><td>3385</td><td>927432</td><td>Gaming Chair</td><td>3</td><td>288.07</td><td>2025-09-14T16:48:16.516Z</td><td>Furniture</td><td>New York</td><td>apple_pay</td><td>GAMING CHAIR</td><td>furniture</td><td>City: New York</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6349,
         547863,
         "Yoga Mat",
         4,
         46.31,
         "2025-09-14T16:43:49.516Z",
         "Sports",
         "New York",
         "apple_pay",
         "YOGA MAT",
         "sports",
         "City: New York"
        ],
        [
         7198,
         370693,
         "Coffee Maker",
         3,
         76.4,
         "2025-09-14T16:17:52.516Z",
         "Home",
         "Illinois",
         "credit_card",
         "COFFEE MAKER",
         "home",
         "City: Illinois"
        ],
        [
         8588,
         604014,
         "Wireless Headphones",
         4,
         208.24,
         "2025-09-14T16:58:33.516Z",
         "Electronics",
         "California",
         "credit_card",
         "WIRELESS HEADPHONES",
         "electronics",
         "City: California"
        ],
        [
         2851,
         991716,
         "Coffee Maker",
         2,
         87.45,
         "2025-09-14T16:12:32.516Z",
         "Home",
         "California",
         "credit_card",
         "COFFEE MAKER",
         "home",
         "City: California"
        ],
        [
         3385,
         927432,
         "Gaming Chair",
         3,
         288.07,
         "2025-09-14T16:48:16.516Z",
         "Furniture",
         "New York",
         "apple_pay",
         "GAMING CHAIR",
         "furniture",
         "City: New York"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "order_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_upper",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category_lower",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location_formatted",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# String transformations\n",
    "text_transformed_stream = file_stream \\\n",
    "    .withColumn(\"product_upper\", upper(col(\"product_name\"))) \\\n",
    "    .withColumn(\"category_lower\", lower(col(\"category\"))) \\\n",
    "    .withColumn(\"location_formatted\", concat(lit(\"City: \"), col(\"customer_location\")))\n",
    "\n",
    "print(\"✅ Text transformations: Upper/lower case, concatenation\")\n",
    "display(text_transformed_stream.limit(5),checkpointLocation=f\"{data_dir}/text_transformed_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e4e07d-dea3-4401-9049-5f336e5c6e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>order_id</th><th>product_name</th><th>quantity</th><th>price</th><th>order_timestamp</th><th>category</th><th>customer_location</th><th>payment_method</th><th>hour_of_day</th><th>day_of_week</th><th>is_weekend</th><th>order_date</th></tr></thead><tbody><tr><td>6349</td><td>547863</td><td>Yoga Mat</td><td>4</td><td>46.31</td><td>2025-09-14T16:43:49.516Z</td><td>Sports</td><td>New York</td><td>apple_pay</td><td>16</td><td>1</td><td>true</td><td>2025-09-14</td></tr><tr><td>7198</td><td>370693</td><td>Coffee Maker</td><td>3</td><td>76.4</td><td>2025-09-14T16:17:52.516Z</td><td>Home</td><td>Illinois</td><td>credit_card</td><td>16</td><td>1</td><td>true</td><td>2025-09-14</td></tr><tr><td>8588</td><td>604014</td><td>Wireless Headphones</td><td>4</td><td>208.24</td><td>2025-09-14T16:58:33.516Z</td><td>Electronics</td><td>California</td><td>credit_card</td><td>16</td><td>1</td><td>true</td><td>2025-09-14</td></tr><tr><td>2851</td><td>991716</td><td>Coffee Maker</td><td>2</td><td>87.45</td><td>2025-09-14T16:12:32.516Z</td><td>Home</td><td>California</td><td>credit_card</td><td>16</td><td>1</td><td>true</td><td>2025-09-14</td></tr><tr><td>3385</td><td>927432</td><td>Gaming Chair</td><td>3</td><td>288.07</td><td>2025-09-14T16:48:16.516Z</td><td>Furniture</td><td>New York</td><td>apple_pay</td><td>16</td><td>1</td><td>true</td><td>2025-09-14</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6349,
         547863,
         "Yoga Mat",
         4,
         46.31,
         "2025-09-14T16:43:49.516Z",
         "Sports",
         "New York",
         "apple_pay",
         16,
         1,
         true,
         "2025-09-14"
        ],
        [
         7198,
         370693,
         "Coffee Maker",
         3,
         76.4,
         "2025-09-14T16:17:52.516Z",
         "Home",
         "Illinois",
         "credit_card",
         16,
         1,
         true,
         "2025-09-14"
        ],
        [
         8588,
         604014,
         "Wireless Headphones",
         4,
         208.24,
         "2025-09-14T16:58:33.516Z",
         "Electronics",
         "California",
         "credit_card",
         16,
         1,
         true,
         "2025-09-14"
        ],
        [
         2851,
         991716,
         "Coffee Maker",
         2,
         87.45,
         "2025-09-14T16:12:32.516Z",
         "Home",
         "California",
         "credit_card",
         16,
         1,
         true,
         "2025-09-14"
        ],
        [
         3385,
         927432,
         "Gaming Chair",
         3,
         288.07,
         "2025-09-14T16:48:16.516Z",
         "Furniture",
         "New York",
         "apple_pay",
         16,
         1,
         true,
         "2025-09-14"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "order_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hour_of_day",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_weekend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Date/time transformations\n",
    "time_transformed_stream = file_stream \\\n",
    "    .withColumn(\"hour_of_day\", hour(col(\"order_timestamp\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"order_timestamp\"))) \\\n",
    "    .withColumn(\"is_weekend\", dayofweek(col(\"order_timestamp\")).isin([1, 7])) \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_timestamp\")))\n",
    "\n",
    "print(\"✅ Time transformations: Hour, day of week, weekend detection\")\n",
    "display(time_transformed_stream.limit(5), checkpointLocation=f\"{data_dir}/time_transformed_checkpoint\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d3fa7f-922c-416b-b78e-6866f3a7d8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>order_id</th><th>product_name</th><th>quantity</th><th>price</th><th>order_timestamp</th><th>category</th><th>customer_location</th><th>payment_method</th><th>customer_segment</th><th>discount_eligible</th></tr></thead><tbody><tr><td>6349</td><td>547863</td><td>Yoga Mat</td><td>4</td><td>46.31</td><td>2025-09-14T16:43:49.516Z</td><td>Sports</td><td>New York</td><td>apple_pay</td><td>High Value</td><td>false</td></tr><tr><td>7198</td><td>370693</td><td>Coffee Maker</td><td>3</td><td>76.4</td><td>2025-09-14T16:17:52.516Z</td><td>Home</td><td>Illinois</td><td>credit_card</td><td>Standard</td><td>false</td></tr><tr><td>8588</td><td>604014</td><td>Wireless Headphones</td><td>4</td><td>208.24</td><td>2025-09-14T16:58:33.516Z</td><td>Electronics</td><td>California</td><td>credit_card</td><td>High Value</td><td>true</td></tr><tr><td>2851</td><td>991716</td><td>Coffee Maker</td><td>2</td><td>87.45</td><td>2025-09-14T16:12:32.516Z</td><td>Home</td><td>California</td><td>credit_card</td><td>Metro</td><td>false</td></tr><tr><td>3385</td><td>927432</td><td>Gaming Chair</td><td>3</td><td>288.07</td><td>2025-09-14T16:48:16.516Z</td><td>Furniture</td><td>New York</td><td>apple_pay</td><td>Metro</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6349,
         547863,
         "Yoga Mat",
         4,
         46.31,
         "2025-09-14T16:43:49.516Z",
         "Sports",
         "New York",
         "apple_pay",
         "High Value",
         false
        ],
        [
         7198,
         370693,
         "Coffee Maker",
         3,
         76.4,
         "2025-09-14T16:17:52.516Z",
         "Home",
         "Illinois",
         "credit_card",
         "Standard",
         false
        ],
        [
         8588,
         604014,
         "Wireless Headphones",
         4,
         208.24,
         "2025-09-14T16:58:33.516Z",
         "Electronics",
         "California",
         "credit_card",
         "High Value",
         true
        ],
        [
         2851,
         991716,
         "Coffee Maker",
         2,
         87.45,
         "2025-09-14T16:12:32.516Z",
         "Home",
         "California",
         "credit_card",
         "Metro",
         false
        ],
        [
         3385,
         927432,
         "Gaming Chair",
         3,
         288.07,
         "2025-09-14T16:48:16.516Z",
         "Furniture",
         "New York",
         "apple_pay",
         "Metro",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "order_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "discount_eligible",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complex conditional logic\n",
    "business_logic_stream = file_stream \\\n",
    "    .withColumn(\"customer_segment\",\n",
    "        when((col(\"price\") > 500) & (col(\"payment_method\") == \"credit_card\"), \"Premium\")\n",
    "        .when((col(\"quantity\") > 3) | (col(\"category\") == \"Electronics\"), \"High Value\")\n",
    "        .when(col(\"customer_location\").isin([\"New York\", \"California\"]), \"Metro\")\n",
    "        .otherwise(\"Standard\")\n",
    "    ) \\\n",
    "    .withColumn(\"discount_eligible\",\n",
    "        (col(\"quantity\") >= 3) & (col(\"price\") > 100)\n",
    "    )\n",
    "\n",
    "print(\"✅ Business logic: Customer segmentation, discount eligibility\")\n",
    "display(business_logic_stream.limit(5),checkpointLocation=f\"{data_dir}/business_logic_checkpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae44b78e-f390-44c7-a373-26a74593bb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⚙️ Part II: Core Concepts\n",
    "\n",
    "## 5. Triggers - Controlling Processing Timing\n",
    "\n",
    "Triggers determine when micro-batches are processed in streaming applications.\n",
    "\n",
    "# 5.1 Understanding Different Trigger Types\n",
    "\n",
    "⏰ Trigger Types in Structured Streaming  \n",
    "==================================================\n",
    "\n",
    "**1️⃣ Default Trigger:**  \n",
    "   • Processes micro-batches as soon as previous batch completes  \n",
    "   • Highest throughput but variable timing  \n",
    "   • Usage: `.trigger()` (no parameters)\n",
    "\n",
    "**2️⃣ Fixed Interval Trigger:**  \n",
    "   • Processes micro-batches at regular intervals  \n",
    "   • Predictable timing, good for regular reporting  \n",
    "   • Usage: `.trigger(processingTime='10 seconds')`\n",
    "\n",
    "**3️⃣ One-time Trigger:**  \n",
    "   • Processes all available data once and stops  \n",
    "   • Good for batch-like processing of streaming data  \n",
    "   • Usage: `.trigger(once=True)`\n",
    "\n",
    "**4️⃣ Continuous Trigger (Experimental):**  \n",
    "   • Ultra-low latency processing (~1ms)  \n",
    "   • Limited operation support  \n",
    "   • Usage: `.trigger(continuous='1 second')`\n",
    "\n",
    "# 5.2 Practical Trigger Examples\n",
    "\n",
    "\uD83D\uDEE0️ Practical Trigger Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067a9d25-2b8d-4cf9-aba3-e51005fa48c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Started AvailableNow trigger demo\n   • Query Name: trigger_demo_table\n   • Processing: All available data and then stops\n   • Monitor: Check processing_batch_time to see batching\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing queries\n",
    "for q in spark.streams.active:\n",
    "    if q.name and \"trigger_demo\" in q.name:\n",
    "        q.stop()\n",
    "        print(f\"\uD83D\uDED1 Stopped existing query: {q.name}\")\n",
    "\n",
    "# Fixed interval trigger example\n",
    "trigger_demo_stream = file_stream \\\n",
    "    .withColumn(\"processing_batch_time\", current_timestamp()) \\\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"order_id\",\n",
    "        \"product_name\",\n",
    "        \"quantity\",\n",
    "        \"price\",\n",
    "        \"order_timestamp\",\n",
    "        \"category\",\n",
    "        \"customer_location\",\n",
    "        \"payment_method\",\n",
    "        \"processing_batch_time\"\n",
    "    )\n",
    "\n",
    "# Start query with AvailableNow trigger\n",
    "trigger_query = trigger_demo_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"trigger_demo_table\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"/{data_dir}/checkpoint_\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Started AvailableNow trigger demo\")\n",
    "print(\"   • Query Name: trigger_demo_table\")\n",
    "print(\"   • Processing: All available data and then stops\")\n",
    "print(\"   • Monitor: Check processing_batch_time to see batching\")\n",
    "\n",
    "# Let it run briefly to collect some data\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44023e70-806f-4fb6-bdd3-b99c85d0e42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Trigger Behavior Analysis:\n+-----------------------+----------------+--------------------------+--------------------------+\n|processing_batch_time  |records_in_batch|batch_start_time          |batch_end_time            |\n+-----------------------+----------------+--------------------------+--------------------------+\n|2025-09-14 17:13:50.052|5               |2025-09-14 16:04:26.822069|2025-09-14 16:47:23.822069|\n|2025-09-14 17:13:49.318|5               |2025-09-14 16:13:42.619924|2025-09-14 16:51:57.619924|\n|2025-09-14 17:13:48.621|5               |2025-09-14 16:12:32.516373|2025-09-14 16:58:33.516373|\n+-----------------------+----------------+--------------------------+--------------------------+\n\n\n\uD83D\uDCA1 Trigger Selection Guidelines:\n  • Real-time dashboards: Fixed interval (5-30 seconds)\n  • Financial reporting: Fixed interval (1-5 minutes)\n  • ETL pipelines: Default trigger for throughput\n  • Batch migration: One-time trigger\n  • Ultra-low latency: Continuous trigger (experimental)\n\n\uD83D\uDED1 Stopped trigger demo query\n"
     ]
    }
   ],
   "source": [
    "# Analyze trigger behavior\n",
    "print(\"\\n\uD83D\uDCCA Trigger Behavior Analysis:\")\n",
    "batch_analysis = spark.sql(\"\"\"\n",
    "SELECT processing_batch_time, \n",
    "       count(*) as records_in_batch,\n",
    "       min(order_timestamp) as batch_start_time,\n",
    "       max(order_timestamp) as batch_end_time\n",
    "FROM trigger_demo_table \n",
    "GROUP BY processing_batch_time \n",
    "ORDER BY processing_batch_time DESC\n",
    "\"\"\")\n",
    "\n",
    "batch_analysis.show(truncate=False)\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Trigger Selection Guidelines:\")\n",
    "guidelines = {\n",
    "    \"Real-time dashboards\": \"Fixed interval (5-30 seconds)\",\n",
    "    \"Financial reporting\": \"Fixed interval (1-5 minutes)\", \n",
    "    \"ETL pipelines\": \"Default trigger for throughput\",\n",
    "    \"Batch migration\": \"One-time trigger\",\n",
    "    \"Ultra-low latency\": \"Continuous trigger (experimental)\"\n",
    "}\n",
    "\n",
    "for use_case, recommendation in guidelines.items():\n",
    "    print(f\"  • {use_case}: {recommendation}\")\n",
    "\n",
    "# Stop demo query\n",
    "trigger_query.stop()\n",
    "print(\"\\n\uD83D\uDED1 Stopped trigger demo query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65d5d23c-f7f1-47e9-8780-e1f305132b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Output Modes - Data Output Strategies\n",
    "\n",
    "Output modes determine what data gets written to the output sink in each micro-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03d3481-c91b-4068-911b-375581f7e73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6.1 Understanding Output Modes\n",
    "\n",
    "\uD83D\uDCE4 **Output Modes in Structured Streaming**\n",
    "=============================================\n",
    "\n",
    "- **Append**\n",
    "  - *Description*: Only new rows added since last trigger\n",
    "  - *Use Cases*: ETL, filtering, stateless transformations\n",
    "  - *Aggregations*: Only with watermarks (finalized windows)\n",
    "  - *Memory Usage*: Low\n",
    "  - *Default For*: Non-aggregation queries\n",
    "\n",
    "- **Complete**\n",
    "  - *Description*: Entire result table in each micro-batch\n",
    "  - *Use Cases*: Small aggregations, dashboards\n",
    "  - *Aggregations*: All aggregations supported\n",
    "  - *Memory Usage*: High (full table in memory)\n",
    "  - *Default For*: Aggregations without watermarks\n",
    "\n",
    "- **Update**\n",
    "  - *Description*: Only rows that were updated since last trigger\n",
    "  - *Use Cases*: Large aggregations with watermarks\n",
    "  - *Aggregations*: Supported with watermarks\n",
    "  - *Memory Usage*: Medium\n",
    "  - *Default For*: Watermarked aggregations\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28197f7d-e1af-4f1e-88bd-2caa57e9599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\uD83D\uDEE0️ Practical Output Mode Examples:\n\n1️⃣ Append Mode Example:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7269659990446062>, line 22\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m1️⃣ Append Mode Example:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m append_stream \u001B[38;5;241m=\u001B[39m file_stream \\\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m200\u001B[39m) \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     15\u001B[0m append_query \u001B[38;5;241m=\u001B[39m append_stream \\\n",
       "\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_mode_demo_append\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     20\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/streaming_checkpoints/output_mode_demo_append\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n",
       "\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Append mode: Only new filtered records each batch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Example 2: Complete Mode (Aggregations)\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n",
       "\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n",
       "\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n",
       "\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n",
       "\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n",
       "\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n",
       "\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n",
       "\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n",
       "\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n",
       "\u001B[1;32m    656\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n",
       "\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n",
       "\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n",
       "\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    614\u001B[0m )\n",
       "\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n",
       "\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    622\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1305\u001B[0m )\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1763\u001B[0m     ):\n",
       "\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/output_mode_demo_append/offsets to start over.\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n",
       "\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n",
       "\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/output_mode_demo_append/offsets to start over.\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/output_mode_demo_append/offsets to start over."
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "_LEGACY_ERROR_TEMP_1299",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "XXKCM",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7269659990446062>, line 22\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m1️⃣ Append Mode Example:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m append_stream \u001B[38;5;241m=\u001B[39m file_stream \\\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m200\u001B[39m) \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m append_query \u001B[38;5;241m=\u001B[39m append_stream \\\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_mode_demo_append\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/streaming_checkpoints/output_mode_demo_append\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Append mode: Only new filtered records each batch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Example 2: Complete Mode (Aggregations)\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n\u001B[1;32m    656\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    614\u001B[0m )\n\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    622\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1305\u001B[0m )\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1763\u001B[0m     ):\n\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/output_mode_demo_append/offsets to start over.\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.2 Practical Output Mode Examples\n",
    "print(\"\\n\\n\uD83D\uDEE0️ Practical Output Mode Examples:\")\n",
    "\n",
    "# Clean up existing demo queries\n",
    "for q in spark.streams.active:\n",
    "    if q.name and \"output_mode_demo\" in q.name:\n",
    "        q.stop()\n",
    "\n",
    "# Example 1: Append Mode (Stateless)\n",
    "print(\"\\n1️⃣ Append Mode Example:\")\n",
    "append_stream = file_stream \\\n",
    "    .filter(col(\"price\") > 200) \\\n",
    "    .select(\"customer_id\", \"product_name\", \"price\", \"category\", \"order_timestamp\")\n",
    "\n",
    "append_query = append_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"output_mode_demo_append\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",f\"{data_dir}/streaming_checkpoints/output_mode_demo_append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Append mode: Only new filtered records each batch\")\n",
    "\n",
    "# Example 2: Complete Mode (Aggregations)\n",
    "print(\"\\n2️⃣ Complete Mode Example:\")\n",
    "complete_stream = file_stream \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"category\",\n",
    "        \"total_orders\",\n",
    "        round(col(\"total_revenue\"), 2).alias(\"total_revenue\"),\n",
    "        round(col(\"avg_price\"), 2).alias(\"avg_price\")\n",
    "    )\n",
    "\n",
    "complete_query = complete_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"output_mode_demo_complete\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/output_mode_demo_complete\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Complete mode: Entire aggregation result each batch\")\n",
    "\n",
    "# Example 3: Update Mode (Watermarked Aggregations)\n",
    "print(\"\\n3️⃣ Update Mode Example:\")\n",
    "update_stream = file_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_timestamp\"), \"2 minutes\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"window_orders\"),\n",
    "        sum(\"price\").alias(\"window_revenue\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        \"category\",\n",
    "        \"window_orders\",\n",
    "        round(col(\"window_revenue\"), 2).alias(\"window_revenue\")\n",
    "    )\n",
    "\n",
    "update_query = update_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"output_mode_demo_update\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/output_mode_demo_update\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Update mode: Only changed windows each batch\")\n",
    "\n",
    "# Let streams run to collect data\n",
    "print(\"\\n⏳ Collecting data for 30 seconds...\")\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64daec1-027e-44d0-9cdc-b78653deff20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Output Mode Results:\n\n\uD83D\uDD38 Append Mode Results (filtered records):\n+-----------+-------------------+-------+-----------+--------------------+\n|customer_id|       product_name|  price|   category|     order_timestamp|\n+-----------+-------------------+-------+-----------+--------------------+\n|       3459|        MacBook Pro|2058.21|Electronics|2025-09-14 14:43:...|\n|       8983|          iPhone 15|1079.94|Electronics|2025-09-14 14:39:...|\n|       4472|       Gaming Chair| 270.43|  Furniture|2025-09-14 14:38:...|\n|       2795|          iPhone 15| 896.54|Electronics|2025-09-14 14:36:...|\n|       2277|        Smart Watch| 437.23|Electronics|2025-09-14 14:35:...|\n|       7074|       Gaming Chair| 302.02|  Furniture|2025-09-14 14:21:...|\n|       1119|          iPhone 15| 801.74|Electronics|2025-09-14 14:20:...|\n|       1307|        MacBook Pro|1933.09|Electronics|2025-09-14 14:14:...|\n|       6173|Wireless Headphones| 221.48|Electronics|2025-09-14 14:11:...|\n|       8534|       Gaming Chair| 274.82|  Furniture|2025-09-14 13:57:...|\n+-----------+-------------------+-------+-----------+--------------------+\n\n\n\uD83D\uDD38 Complete Mode Results (running totals):\n+-----------+------------+-------------+---------+\n|   category|total_orders|total_revenue|avg_price|\n+-----------+------------+-------------+---------+\n|Electronics|           6|      5877.12|   979.52|\n|  Furniture|           2|       572.45|   286.23|\n|    Fashion|           1|       104.64|   104.64|\n|       Home|           1|        101.2|    101.2|\n+-----------+------------+-------------+---------+\n\n\n\uD83D\uDD38 Update Mode Results (windowed aggregations):\n+-------------------+-------------------+-----------+-------------+--------------+\n|       window_start|         window_end|   category|window_orders|window_revenue|\n+-------------------+-------------------+-----------+-------------+--------------+\n|2025-09-14 14:54:00|2025-09-14 14:56:00|       Home|            1|         101.2|\n|2025-09-14 14:42:00|2025-09-14 14:44:00|Electronics|            1|       2058.21|\n|2025-09-14 14:38:00|2025-09-14 14:40:00|  Furniture|            1|        270.43|\n|2025-09-14 14:34:00|2025-09-14 14:36:00|Electronics|            1|        437.23|\n|2025-09-14 14:22:00|2025-09-14 14:24:00|    Fashion|            1|        104.64|\n|2025-09-14 14:20:00|2025-09-14 14:22:00|Electronics|            1|        801.74|\n|2025-09-14 14:20:00|2025-09-14 14:22:00|  Furniture|            1|        302.02|\n|2025-09-14 14:14:00|2025-09-14 14:16:00|Electronics|            1|       1933.09|\n|2025-09-14 14:10:00|2025-09-14 14:12:00|Electronics|            1|        221.48|\n|2025-09-14 13:56:00|2025-09-14 13:58:00|Electronics|            1|        425.37|\n+-------------------+-------------------+-----------+-------------+--------------+\n\n\n\uD83D\uDED1 Stopped all output mode demo queries\n\n\uD83D\uDCA1 Output Mode Selection Guide:\n  • Append: ETL pipelines, data ingestion, stateless processing\n  • Complete: Real-time dashboards with small result sets\n  • Update: Large-scale aggregations with time windows\n"
     ]
    }
   ],
   "source": [
    "# Show results from different output modes\n",
    "print(\"\\n\uD83D\uDCCA Output Mode Results:\")\n",
    "\n",
    "print(\"\\n\uD83D\uDD38 Append Mode Results (filtered records):\")\n",
    "spark.sql(\"SELECT * FROM output_mode_demo_append ORDER BY order_timestamp DESC LIMIT 10\").show()\n",
    "\n",
    "print(\"\\n\uD83D\uDD38 Complete Mode Results (running totals):\")\n",
    "spark.sql(\"SELECT * FROM output_mode_demo_complete ORDER BY total_revenue DESC\").show()\n",
    "\n",
    "print(\"\\n\uD83D\uDD38 Update Mode Results (windowed aggregations):\")\n",
    "spark.sql(\"SELECT * FROM output_mode_demo_update ORDER BY window_start DESC LIMIT 10\").show()\n",
    "\n",
    "# Clean up\n",
    "for query in [append_query, complete_query, update_query]:\n",
    "    if query.isActive:\n",
    "        query.stop()\n",
    "\n",
    "print(\"\\n\uD83D\uDED1 Stopped all output mode demo queries\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Output Mode Selection Guide:\")\n",
    "print(\"  • Append: ETL pipelines, data ingestion, stateless processing\")\n",
    "print(\"  • Complete: Real-time dashboards with small result sets\")\n",
    "print(\"  • Update: Large-scale aggregations with time windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84b4705e-5c6a-4b4e-bafa-6055b50c8e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. WriteStream Operations and Sinks\n",
    "\n",
    "Understanding different output sinks and their configurations for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf10edd-4384-41fd-92cd-1658356a8284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7.1 Output Sink Types and Configurations\n",
    "print(\"\uD83C\uDFAF WriteStream Operations and Sinks\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Helper functions for different sinks\n",
    "def start_console_sink(dataframe, query_name, output_mode=\"append\", trigger_interval=\"10 seconds\", num_rows=20):\n",
    "    \"\"\"Console sink for development and debugging\"\"\"\n",
    "    return dataframe.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .option(\"numRows\", num_rows) \\\n",
    "        .trigger(processingTime=trigger_interval) \\\n",
    "        .queryName(query_name) \\\n",
    "        .start()\n",
    "\n",
    "def start_memory_sink(dataframe, table_name, output_mode=\"append\", trigger_interval=\"10 seconds\"):\n",
    "    \"\"\"Memory sink for testing and interactive analysis\"\"\"\n",
    "    return dataframe.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(table_name) \\\n",
    "        .trigger(processingTime=trigger_interval) \\\n",
    "        .start()\n",
    "\n",
    "def start_file_sink(dataframe, output_path, file_format=\"parquet\", output_mode=\"append\", \n",
    "                   trigger_interval=\"30 seconds\", query_name=\"file_sink\"):\n",
    "    \"\"\"File sink for data lake storage\"\"\"\n",
    "    return dataframe.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .format(file_format) \\\n",
    "        .option(\"path\", output_path) \\\n",
    "        .option(\"checkpointLocation\", f\"/tmp/checkpoints/{query_name}\") \\\n",
    "        .trigger(processingTime=trigger_interval) \\\n",
    "        .queryName(query_name) \\\n",
    "        .start()\n",
    "\n",
    "print(\"\uD83D\uDEE0️ Sink helper functions created:\")\n",
    "print(\"  • start_console_sink() - For debugging and development\")\n",
    "print(\"  • start_memory_sink() - For testing and analysis\")\n",
    "print(\"  • start_file_sink() - For data lake storage\")\n",
    "\n",
    "# 7.2 Practical Sink Examples\n",
    "print(\"\\n\uD83D\uDCCA Practical Sink Implementations:\")\n",
    "\n",
    "# Clean up existing queries\n",
    "for q in spark.streams.active:\n",
    "    if q.name and \"sink_demo\" in q.name:\n",
    "        q.stop()\n",
    "\n",
    "# Prepare different data streams for sink demos\n",
    "raw_data_stream = enhanced_rate_stream.select(\n",
    "    \"customer_id\", \"product_name\", \"price\", \"category\", \"timestamp\"\n",
    ")\n",
    "\n",
    "aggregated_stream = enhanced_rate_stream \\\n",
    "    .groupBy(\"category\", \"customer_location\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"category\", \"customer_location\", \"order_count\",\n",
    "        round(col(\"total_revenue\"), 2).alias(\"total_revenue\"),\n",
    "        round(col(\"avg_order_value\"), 2).alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "# Example 1: Console Sink (Development)\n",
    "print(\"\\n1️⃣ Console Sink - Live data preview:\")\n",
    "console_query = start_console_sink(\n",
    "    raw_data_stream.filter(col(\"price\") > 300),\n",
    "    \"sink_demo_console\",\n",
    "    output_mode=\"append\",\n",
    "    trigger_interval=\"8 seconds\",\n",
    "    num_rows=5\n",
    ")\n",
    "print(\"✅ Console sink started - High-value orders preview\")\n",
    "\n",
    "# Example 2: Memory Sink (Interactive Analysis)\n",
    "print(\"\\n2️⃣ Memory Sink - For SQL queries:\")\n",
    "memory_query = start_memory_sink(\n",
    "    aggregated_stream,\n",
    "    \"sink_demo_memory\",\n",
    "    output_mode=\"complete\",\n",
    "    trigger_interval=\"12 seconds\"\n",
    ")\n",
    "print(\"✅ Memory sink started - Category analytics table\")\n",
    "\n",
    "# Example 3: File Sink (Data Storage)\n",
    "print(\"\\n3️⃣ File Sink - Persistent storage:\")\n",
    "output_path = \"/tmp/streaming_output/orders\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "file_query = start_file_sink(\n",
    "    raw_data_stream,\n",
    "    output_path,\n",
    "    file_format=\"json\",\n",
    "    output_mode=\"append\",\n",
    "    trigger_interval=\"20 seconds\",\n",
    "    query_name=\"sink_demo_file\"\n",
    ")\n",
    "print(f\"✅ File sink started - Saving to {output_path}\")\n",
    "\n",
    "# Let sinks run and collect data\n",
    "print(\"\\n⏳ Running sinks for 40 seconds to collect data...\")\n",
    "time.sleep(40)\n",
    "\n",
    "# Query memory sink results\n",
    "print(\"\\n\uD83D\uDCCB Memory Sink Results:\")\n",
    "memory_results = spark.sql(\"\"\"\n",
    "SELECT category, customer_location, order_count, total_revenue, avg_order_value\n",
    "FROM sink_demo_memory \n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "memory_results.show()\n",
    "\n",
    "# Check file sink output\n",
    "print(\"\\n\uD83D\uDCC1 File Sink Output:\")\n",
    "if os.path.exists(output_path) and os.listdir(output_path):\n",
    "    files = [f for f in os.listdir(output_path) if f.endswith('.json')]\n",
    "    print(f\"Created {len(files)} JSON files in {output_path}\")\n",
    "    if files:\n",
    "        sample_file = os.path.join(output_path, files[0])\n",
    "        with open(sample_file, 'r') as f:\n",
    "            sample_content = f.readline()\n",
    "        print(f\"Sample content: {sample_content[:100]}...\")\n",
    "else:\n",
    "    print(\"No files created yet (may need more time)\")\n",
    "\n",
    "# Query monitoring\n",
    "print(\"\\n\uD83D\uDCCA Active Query Status:\")\n",
    "for query in [console_query, memory_query, file_query]:\n",
    "    if query.isActive:\n",
    "        progress = query.lastProgress\n",
    "        print(f\"  • {query.name}: Active\")\n",
    "        if progress:\n",
    "            print(f\"    Batch: {progress.get('batchId', 'N/A')}\")\n",
    "            print(f\"    Input Rate: {progress.get('inputRowsPerSecond', 0):.1f} rows/sec\")\n",
    "    else:\n",
    "        print(f\"  • {query.name}: Stopped\")\n",
    "\n",
    "# Clean up queries\n",
    "for query in [console_query, memory_query, file_query]:\n",
    "    if query.isActive:\n",
    "        query.stop()\n",
    "\n",
    "print(\"\\n\uD83D\uDED1 Stopped all sink demo queries\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Sink Selection Guidelines:\")\n",
    "sink_guidelines = {\n",
    "    \"Development/Debug\": \"Console sink\",\n",
    "    \"Interactive Analysis\": \"Memory sink\", \n",
    "    \"Data Lake Storage\": \"File sink (Parquet/Delta)\",\n",
    "    \"Real-time Apps\": \"Kafka sink\",\n",
    "    \"Databases\": \"JDBC sink (via foreachBatch)\",\n",
    "    \"Custom Logic\": \"Custom sink (via foreach/foreachBatch)\"\n",
    "}\n",
    "\n",
    "for use_case, sink_type in sink_guidelines.items():\n",
    "    print(f\"  • {use_case}: {sink_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "facefb73-5b11-4448-b697-2ba9d286f57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Basic Aggregations and Windowing\n",
    "\n",
    "Introduction to stateful operations and time-based aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65106829-6e2b-4728-8cda-060e242e9e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: long (nullable = true)\n |-- order_id: long (nullable = true)\n |-- product_name: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- order_timestamp: timestamp (nullable = true)\n |-- category: string (nullable = true)\n |-- customer_location: string (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- customer_segment: string (nullable = false)\n |-- discount_eligible: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "business_logic_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64b81e6-7a1c-4fe4-9017-530e355d86d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Setting up aggregations...\n✅ Aggregation queries defined\n\n\uD83D\uDD04 Starting aggregation demos...\n"
     ]
    }
   ],
   "source": [
    "# Create aggregation queries\n",
    "print(\"\uD83D\uDCCA Setting up aggregations...\")\n",
    "\n",
    "# Product performance aggregation\n",
    "product_agg = business_logic_stream \\\n",
    "    .groupBy(\"product_name\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        avg(\"price\").alias(\"avg_price\"),\n",
    "        approx_count_distinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_price\", round(col(\"avg_price\"), 2)) \\\n",
    "    .withColumn(\"total_revenue\", round(col(\"total_revenue\"), 2))\n",
    "\n",
    "# Location-based aggregation\n",
    "location_agg = business_logic_stream \\\n",
    "    .groupBy(\"customer_location\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        sum(col(\"price\") * col(\"quantity\")).alias(\"total_sales\"),\n",
    "        approx_count_distinct(\"customer_id\").alias(\"active_customers\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_order_value\", \n",
    "        round(col(\"total_sales\") / col(\"total_orders\"), 2)\n",
    "    ) \\\n",
    "    .withColumnRenamed(\"customer_location\", \"location\")\n",
    "\n",
    "print(\"✅ Aggregation queries defined\")\n",
    "\n",
    "# Start aggregation streams\n",
    "print(\"\\n\uD83D\uDD04 Starting aggregation demos...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6272f9e-7da1-46af-86bc-3110415a3a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCC8 Product Performance:\n+-------------------+-----------+-----------+-------------+---------+----------------+\n|       product_name|   category|order_count|total_revenue|avg_price|unique_customers|\n+-------------------+-----------+-----------+-------------+---------+----------------+\n|        MacBook Pro|Electronics|          2|       3991.3|  1995.65|               2|\n|        MacBook Pro|Electronics|          1|      2058.21|  2058.21|               1|\n|        Smart Watch|Electronics|          2|        862.6|    431.3|               2|\n|          iPhone 15|Electronics|          1|       801.74|   801.74|               1|\n|       Gaming Chair|  Furniture|          2|       572.45|   286.23|               2|\n|        Smart Watch|Electronics|          1|       425.37|   425.37|               1|\n|       Gaming Chair|  Furniture|          1|       270.43|   270.43|               1|\n|Wireless Headphones|Electronics|          1|       221.48|   221.48|               1|\n|       Nike Air Max|    Fashion|          1|       104.64|   104.64|               1|\n|       Coffee Maker|       Home|          1|        101.2|    101.2|               1|\n+-------------------+-----------+-----------+-------------+---------+----------------+\n\n\n\uD83D\uDDFA️ Location Performance:\n+----------+------------+------------------+----------------+---------------+\n|  location|total_orders|       total_sales|active_customers|avg_order_value|\n+----------+------------+------------------+----------------+---------------+\n|California|           3|          14224.56|               3|        4741.52|\n|  New York|           2|            6600.0|               2|         3300.0|\n|California|           2| 4559.110000000001|               2|        2279.56|\n|  Illinois|           2|2315.4799999999996|               2|        1157.74|\n|   Florida|           2|            614.08|               2|         307.04|\n|     Texas|           1|            437.23|               1|         437.23|\n|   Florida|           1|            209.28|               1|         209.28|\n+----------+------------+------------------+----------------+---------------+\n\n\n✅ Aggregation demos complete\n"
     ]
    }
   ],
   "source": [
    "product_query = product_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"product_performance\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/product_performance\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "location_query = location_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"location_performance\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/location_performance\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Let them run\n",
    "time.sleep(25)\n",
    "\n",
    "# Show results\n",
    "print(\"\\n\uD83D\uDCC8 Product Performance:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_name, category, order_count, total_revenue, avg_price, unique_customers\n",
    "    FROM product_performance \n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n\uD83D\uDDFA️ Location Performance:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT location, total_orders, total_sales, active_customers, avg_order_value\n",
    "    FROM location_performance \n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Clean up\n",
    "product_query.stop()\n",
    "location_query.stop()\n",
    "print(\"\\n✅ Aggregation demos complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a533d17-5c98-4571-98df-0af963c8018b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDD25 Part III: Advanced Features\n",
    "\n",
    "## 9. Window Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7432bf72-ad99-4d67-bf91-21f1d5784a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDE9F Window Operations Overview:\n  • Tumbling Windows: Fixed, non-overlapping intervals\n  • Sliding Windows: Fixed size, overlapping intervals\n  • Session Windows: Dynamic, activity-based intervals\n\n✅ Tumbling window configured (2-minute intervals)\n✅ Sliding window configured (5-minute windows, 2-minute slides)\n\n\uD83D\uDD04 Starting window operation demos...\n\n\uD83D\uDCCA Tumbling Window Results (2-minute intervals):\n+-------------------+-------------------+-----------+-----------+-------+----------------+\n|window_start       |window_end         |category   |order_count|revenue|unique_customers|\n+-------------------+-------------------+-----------+-----------+-------+----------------+\n|2025-09-14 14:42:00|2025-09-14 14:44:00|Electronics|1          |2058.21|1               |\n|2025-09-14 14:38:00|2025-09-14 14:40:00|Furniture  |1          |270.43 |1               |\n|2025-09-14 14:34:00|2025-09-14 14:36:00|Electronics|1          |437.23 |1               |\n|2025-09-14 14:22:00|2025-09-14 14:24:00|Fashion    |1          |104.64 |1               |\n|2025-09-14 14:20:00|2025-09-14 14:22:00|Electronics|1          |801.74 |1               |\n|2025-09-14 14:20:00|2025-09-14 14:22:00|Furniture  |1          |302.02 |1               |\n|2025-09-14 14:14:00|2025-09-14 14:16:00|Electronics|1          |1933.09|1               |\n|2025-09-14 14:10:00|2025-09-14 14:12:00|Electronics|1          |221.48 |1               |\n|2025-09-14 13:56:00|2025-09-14 13:58:00|Electronics|1          |425.37 |1               |\n+-------------------+-------------------+-----------+-----------+-------+----------------+\n\n\n\uD83D\uDCC8 Sliding Window Results (5-min windows, 2-min slides):\n+-------------------+----------+-----------+---------+\n|window_start       |location  |order_count|avg_price|\n+-------------------+----------+-----------+---------+\n|2025-09-14 14:42:00|New York  |1          |2058.21  |\n|2025-09-14 14:40:00|New York  |1          |2058.21  |\n|2025-09-14 14:38:00|New York  |1          |1079.94  |\n|2025-09-14 14:38:00|California|1          |270.43   |\n|2025-09-14 14:36:00|New York  |1          |1079.94  |\n|2025-09-14 14:36:00|Illinois  |1          |896.54   |\n|2025-09-14 14:36:00|California|1          |270.43   |\n|2025-09-14 14:34:00|Texas     |1          |437.23   |\n|2025-09-14 14:34:00|California|1          |270.43   |\n|2025-09-14 14:32:00|Texas     |1          |437.23   |\n+-------------------+----------+-----------+---------+\n\n\n✅ Window operation demos complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83E\uDE9F Window Operations Overview:\")\n",
    "print(\"  • Tumbling Windows: Fixed, non-overlapping intervals\")\n",
    "print(\"  • Sliding Windows: Fixed size, overlapping intervals\")\n",
    "print(\"  • Session Windows: Dynamic, activity-based intervals\")\n",
    "\n",
    "# Tumbling window: 2-minute windows\n",
    "tumbling_sales = business_logic_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_timestamp\"), \"2 minutes\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"price\").alias(\"revenue\"),\n",
    "        approx_count_distinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        \"category\",\n",
    "        \"order_count\",\n",
    "        round(col(\"revenue\"), 2).alias(\"revenue\"),\n",
    "        \"unique_customers\"\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Tumbling window configured (2-minute intervals)\")\n",
    "\n",
    "# Sliding window: 5-minute windows every 2 minutes\n",
    "sliding_trends = business_logic_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"3 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_timestamp\"), \"5 minutes\", \"2 minutes\"),\n",
    "        col(\"customer_location\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"customer_location\").alias(\"location\"),\n",
    "        \"order_count\",\n",
    "        round(col(\"avg_price\"), 2).alias(\"avg_price\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Sliding window configured (5-minute windows, 2-minute slides)\")\n",
    "\n",
    "# Start window operations\n",
    "print(\"\\n\uD83D\uDD04 Starting window operation demos...\")\n",
    "tumbling_query = tumbling_sales \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"tumbling_sales\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/tumbling_sales\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "sliding_query = sliding_trends \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"sliding_trends\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/sliding_trends\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Let them run\n",
    "time.sleep(35)\n",
    "\n",
    "# Show results\n",
    "print(\"\\n\uD83D\uDCCA Tumbling Window Results (2-minute intervals):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window_start, window_end, category, order_count, revenue, unique_customers\n",
    "    FROM tumbling_sales \n",
    "    ORDER BY window_start DESC, revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\uD83D\uDCC8 Sliding Window Results (5-min windows, 2-min slides):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window_start, location, order_count, avg_price\n",
    "    FROM sliding_trends \n",
    "    ORDER BY window_start DESC, avg_price DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Clean up\n",
    "tumbling_query.stop()\n",
    "sliding_query.stop()\n",
    "print(\"\\n✅ Window operation demos complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed63c06-4955-4a30-9a56-d08b42b60a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Watermarks and Late Data\n",
    "\n",
    "**Understanding Watermarks:**\n",
    "  • Define how late data can arrive before being dropped  \n",
    "  • Enable automatic cleanup of old state  \n",
    "  • Balance between data completeness and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d620157-4897-4925-98c8-d4862decacb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Conservative watermark: 5 minutes (high completeness)\n✅ Aggressive watermark: 30 seconds (low latency)\n\n\uD83D\uDD04 Starting watermark comparison...\n\n\uD83D\uDCCA Conservative Watermark Results (5-minute tolerance):\n+-------------------+-----------+-----------+-------+\n|window_start       |category   |order_count|revenue|\n+-------------------+-----------+-----------+-------+\n|2025-09-14 14:42:00|Electronics|1          |2058.21|\n|2025-09-14 14:39:00|Electronics|1          |1079.94|\n|2025-09-14 14:36:00|Electronics|1          |896.54 |\n|2025-09-14 14:36:00|Furniture  |1          |270.43 |\n|2025-09-14 14:33:00|Electronics|1          |437.23 |\n|2025-09-14 14:21:00|Furniture  |1          |302.02 |\n|2025-09-14 14:21:00|Fashion    |1          |104.64 |\n|2025-09-14 14:18:00|Electronics|1          |801.74 |\n+-------------------+-----------+-----------+-------+\n\n\n⚡ Aggressive Watermark Results (30-second tolerance):\n+-------------------+----------+-----------+---------+\n|window_start       |location  |order_count|avg_price|\n+-------------------+----------+-----------+---------+\n|2025-09-14 14:43:00|New York  |1          |2058.21  |\n|2025-09-14 14:38:00|California|1          |270.43   |\n|2025-09-14 14:35:00|Texas     |1          |437.23   |\n|2025-09-14 14:23:00|Florida   |1          |104.64   |\n|2025-09-14 14:21:00|Illinois  |1          |302.02   |\n|2025-09-14 14:20:00|California|1          |801.74   |\n|2025-09-14 14:14:00|California|1          |1933.09  |\n|2025-09-14 14:11:00|Illinois  |1          |221.48   |\n+-------------------+----------+-----------+---------+\n\n\n✅ Watermark comparison complete\n"
     ]
    }
   ],
   "source": [
    "# Conservative watermark (5 minutes)\n",
    "conservative_agg = business_logic_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_timestamp\"), \"3 minutes\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"price\").alias(\"revenue\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        \"category\",\n",
    "        \"order_count\",\n",
    "        round(col(\"revenue\"), 2).alias(\"revenue\")\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Conservative watermark: 5 minutes (high completeness)\")\n",
    "\n",
    "# Aggressive watermark (30 seconds)\n",
    "aggressive_agg = business_logic_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_timestamp\"), \"1 minute\"),\n",
    "        col(\"customer_location\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"customer_location\").alias(\"location\"),\n",
    "        \"order_count\",\n",
    "        round(col(\"avg_price\"), 2).alias(\"avg_price\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Aggressive watermark: 30 seconds (low latency)\")\n",
    "\n",
    "# Start watermark comparison\n",
    "print(\"\\n\uD83D\uDD04 Starting watermark comparison...\")\n",
    "\n",
    "conservative_query = conservative_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"conservative_watermark\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/conservative_watermark\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "aggressive_query = aggressive_agg \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"aggressive_watermark\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/aggressive_watermark\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Let them run\n",
    "time.sleep(30)\n",
    "\n",
    "# Show results\n",
    "print(\"\\n\uD83D\uDCCA Conservative Watermark Results (5-minute tolerance):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window_start, category, order_count, revenue\n",
    "    FROM conservative_watermark \n",
    "    ORDER BY window_start DESC, revenue DESC\n",
    "    LIMIT 8\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n⚡ Aggressive Watermark Results (30-second tolerance):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window_start, location, order_count, avg_price\n",
    "    FROM aggressive_watermark \n",
    "    ORDER BY window_start DESC, avg_price DESC\n",
    "    LIMIT 8\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Clean up\n",
    "conservative_query.stop()\n",
    "aggressive_query.stop()\n",
    "print(\"\\n✅ Watermark comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "155539f5-a7ae-45fd-9ce8-453df8af9ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Streaming Joins\n",
    "\n",
    "Streaming joins allow you to combine streaming data with static data or other streams for enrichment and complex analytics.\n",
    "\n",
    "\uD83D\uDD17 **Streaming Joins Overview:**\n",
    "  • Stream-to-Static: Join streaming data with static reference tables  \n",
    "  • Stream-to-Stream: Join two streaming datasets  \n",
    "  • Inner/Outer Joins: Different join types for various use cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06db71e1-5a88-49db-9588-d340ad2ce576",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Streaming Joins Overview:\n  • Stream-to-Static: Join streaming data with static reference tables\n  • Stream-to-Stream: Join two streaming datasets\n  • Inner/Outer Joins: Different join types for various use cases\n\n\uD83D\uDCCB Product Catalog (Static Reference Data):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_name</th><th>catalog_category</th><th>brand</th><th>list_price</th><th>tier</th></tr></thead><tbody><tr><td>iPhone 15</td><td>Electronics</td><td>Apple</td><td>999.99</td><td>Premium</td></tr><tr><td>MacBook Pro</td><td>Electronics</td><td>Apple</td><td>1999.99</td><td>Premium</td></tr><tr><td>Nike Shoes</td><td>Fashion</td><td>Nike</td><td>129.99</td><td>Standard</td></tr><tr><td>Coffee Maker</td><td>Home</td><td>Breville</td><td>89.99</td><td>Standard</td></tr><tr><td>Headphones</td><td>Electronics</td><td>Sony</td><td>199.99</td><td>Standard</td></tr><tr><td>Gaming Chair</td><td>Furniture</td><td>Herman Miller</td><td>299.99</td><td>Premium</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "iPhone 15",
         "Electronics",
         "Apple",
         999.99,
         "Premium"
        ],
        [
         "MacBook Pro",
         "Electronics",
         "Apple",
         1999.99,
         "Premium"
        ],
        [
         "Nike Shoes",
         "Fashion",
         "Nike",
         129.99,
         "Standard"
        ],
        [
         "Coffee Maker",
         "Home",
         "Breville",
         89.99,
         "Standard"
        ],
        [
         "Headphones",
         "Electronics",
         "Sony",
         199.99,
         "Standard"
        ],
        [
         "Gaming Chair",
         "Furniture",
         "Herman Miller",
         299.99,
         "Premium"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "catalog_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "brand",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "list_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tier",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1️⃣ Stream-to-Static Join Example:\n✅ Stream-to-static join: Orders enriched with product catalog data\n\n2️⃣ Stream-to-Stream Join Example:\n✅ Stream-to-stream join: Orders correlated with customer behavior events\n\n\uD83D\uDD04 Starting streaming joins demos...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7269659990446090>, line 112\u001B[0m\n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDD04 Starting streaming joins demos...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# Stream-to-static join query\u001B[39;00m\n",
       "\u001B[1;32m    105\u001B[0m enriched_query \u001B[38;5;241m=\u001B[39m enriched_orders \\\n",
       "\u001B[1;32m    106\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m    107\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m    108\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menriched_orders\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m    109\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m    110\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/streaming_checkpoints/enriched_query\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m    111\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n",
       "\u001B[0;32m--> 112\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# Stream-to-stream join query\u001B[39;00m\n",
       "\u001B[1;32m    115\u001B[0m correlation_query \u001B[38;5;241m=\u001B[39m customer_order_correlation \\\n",
       "\u001B[1;32m    116\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m    117\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    121\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n",
       "\u001B[1;32m    122\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n",
       "\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n",
       "\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n",
       "\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n",
       "\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n",
       "\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n",
       "\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n",
       "\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n",
       "\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n",
       "\u001B[1;32m    656\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n",
       "\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n",
       "\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n",
       "\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    614\u001B[0m )\n",
       "\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n",
       "\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    622\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1305\u001B[0m )\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1763\u001B[0m     ):\n",
       "\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/enriched_query/offsets to start over.\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n",
       "\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:98)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n",
       "\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n",
       "\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/enriched_query/offsets to start over.\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:98)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/enriched_query/offsets to start over."
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "_LEGACY_ERROR_TEMP_1299",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "XXKCM",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:98)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7269659990446090>, line 112\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDD04 Starting streaming joins demos...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# Stream-to-static join query\u001B[39;00m\n\u001B[1;32m    105\u001B[0m enriched_query \u001B[38;5;241m=\u001B[39m enriched_orders \\\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menriched_orders\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/streaming_checkpoints/enriched_query\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n\u001B[0;32m--> 112\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# Stream-to-stream join query\u001B[39;00m\n\u001B[1;32m    115\u001B[0m correlation_query \u001B[38;5;241m=\u001B[39m customer_order_correlation \\\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \\\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:648\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    641\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    647\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n\u001B[0;32m--> 648\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n\u001B[1;32m    649\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m    650\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    652\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n\u001B[1;32m    653\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n\u001B[1;32m    654\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n\u001B[1;32m    655\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n\u001B[1;32m    656\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/streaming/readwriter.py:610\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n\u001B[1;32m    609\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m--> 610\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n\u001B[1;32m    612\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m    613\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    614\u001B[0m )\n\u001B[1;32m    615\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n\u001B[1;32m    616\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[1;32m    617\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    621\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    622\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1305\u001B[0m )\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1763\u001B[0m     ):\n\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: This query does not support recovering from checkpoint location. Delete /Volumes/workspace/default/stream/spark_streaming_workshop/streaming_checkpoints/enriched_query/offsets to start over.\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:4313)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:144)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:50)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:45)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:338)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:583)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:583)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:574)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:505)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:388)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:561)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:561)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:554)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:655)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:98)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:408)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:516)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:475)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:381)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:179)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:71)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4014)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3209)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\uD83D\uDD17 Streaming Joins Overview:\")\n",
    "print(\"  • Stream-to-Static: Join streaming data with static reference tables\")\n",
    "print(\"  • Stream-to-Stream: Join two streaming datasets\")\n",
    "print(\"  • Inner/Outer Joins: Different join types for various use cases\")\n",
    "\n",
    "# Create static reference data (product catalog)\n",
    "product_catalog_data = [\n",
    "    (\"iPhone 15\", \"Electronics\", \"Apple\", 999.99, \"Premium\"),\n",
    "    (\"MacBook Pro\", \"Electronics\", \"Apple\", 1999.99, \"Premium\"),\n",
    "    (\"Nike Shoes\", \"Fashion\", \"Nike\", 129.99, \"Standard\"),\n",
    "    (\"Coffee Maker\", \"Home\", \"Breville\", 89.99, \"Standard\"),\n",
    "    (\"Headphones\", \"Electronics\", \"Sony\", 199.99, \"Standard\"),\n",
    "    (\"Gaming Chair\", \"Furniture\", \"Herman Miller\", 299.99, \"Premium\")\n",
    "]\n",
    "\n",
    "product_catalog_schema = StructType([\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"catalog_category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"list_price\", DoubleType(), True),\n",
    "    StructField(\"tier\", StringType(), True)\n",
    "])\n",
    "\n",
    "product_catalog = spark.createDataFrame(product_catalog_data, product_catalog_schema)\n",
    "product_catalog.createOrReplaceTempView(\"product_catalog\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB Product Catalog (Static Reference Data):\")\n",
    "display(product_catalog)\n",
    "\n",
    "# Stream-to-Static Join: Enrich orders with product details\n",
    "print(\"\\n1️⃣ Stream-to-Static Join Example:\")\n",
    "\n",
    "enriched_orders = business_logic_stream \\\n",
    "    .withColumnRenamed(\"category\", \"order_category\") \\\n",
    "    .join(product_catalog, \"product_name\", \"left\") \\\n",
    "    .withColumn(\"discount_amount\", col(\"list_price\") - col(\"price\")) \\\n",
    "    .withColumn(\"discount_percentage\", \n",
    "        round(((col(\"list_price\") - col(\"price\")) / col(\"list_price\")) * 100, 2)\n",
    "    ) \\\n",
    "    .withColumn(\"total_value\", col(\"price\") * col(\"quantity\")) \\\n",
    "    .select(\n",
    "        \"customer_id\", \"order_id\", \"product_name\", \"brand\", \"tier\",\n",
    "        \"price\", \"list_price\", \"discount_amount\", \"discount_percentage\",\n",
    "        \"quantity\", \"total_value\", \"order_category\", \"catalog_category\", \"customer_location\", \"order_timestamp\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Stream-to-static join: Orders enriched with product catalog data\")\n",
    "\n",
    "# Create a second stream for customer behavior tracking\n",
    "customer_behavior_stream = file_stream \\\n",
    "    .withColumn(\"customer_id\", (col(\"value\") % 100) + 1001) \\\n",
    "    .withColumn(\"event_type\", \n",
    "        when(col(\"value\") % 4 == 0, \"page_view\")\n",
    "        .when(col(\"value\") % 4 == 1, \"add_to_cart\")\n",
    "        .when(col(\"value\") % 4 == 2, \"remove_from_cart\")\n",
    "        .otherwise(\"wishlist_add\")\n",
    "    ) \\\n",
    "    .withColumn(\"page_category\", \n",
    "        when(col(\"value\") % 5 == 0, \"Electronics\")\n",
    "        .when(col(\"value\") % 5 == 1, \"Fashion\")\n",
    "        .when(col(\"value\") % 5 == 2, \"Home\")\n",
    "        .when(col(\"value\") % 5 == 3, \"Furniture\")\n",
    "        .otherwise(\"Sports\")\n",
    "    ) \\\n",
    "    .withColumn(\"session_id\", (col(\"value\") / 10).cast(\"int\")) \\\n",
    "    .select(\"customer_id\", \"event_type\", \"page_category\", \"session_id\", \"timestamp\") \\\n",
    "    .withColumnRenamed(\"timestamp\", \"event_timestamp\")\n",
    "\n",
    "print(\"\\n2️⃣ Stream-to-Stream Join Example:\")\n",
    "\n",
    "# Stream-to-Stream Join: Correlate orders with customer behavior\n",
    "customer_order_correlation = business_logic_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"10 minutes\") \\\n",
    "    .alias(\"orders\") \\\n",
    "    .join(\n",
    "        customer_behavior_stream\n",
    "            .withWatermark(\"event_timestamp\", \"10 minutes\")\n",
    "            .alias(\"behavior\"),\n",
    "        col(\"orders.customer_id\") == col(\"behavior.customer_id\"),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .where(\n",
    "        col(\"orders.order_timestamp\") >= col(\"behavior.event_timestamp\") - expr(\"INTERVAL 5 MINUTES\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        col(\"orders.order_timestamp\") <= col(\"behavior.event_timestamp\") + expr(\"INTERVAL 2 MINUTES\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"orders.customer_id\"),\n",
    "        col(\"orders.product_name\"),\n",
    "        col(\"orders.price\"),\n",
    "        col(\"orders.order_category\"),\n",
    "        col(\"behavior.event_type\"),\n",
    "        col(\"behavior.page_category\"),\n",
    "        col(\"orders.order_timestamp\").alias(\"order_time\"),\n",
    "        col(\"behavior.event_timestamp\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Stream-to-stream join: Orders correlated with customer behavior events\")\n",
    "\n",
    "# Start join demonstrations\n",
    "print(\"\\n\uD83D\uDD04 Starting streaming joins demos...\")\n",
    "\n",
    "# Stream-to-static join query\n",
    "enriched_query = enriched_orders \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"enriched_orders\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/enriched_query\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Stream-to-stream join query\n",
    "correlation_query = customer_order_correlation \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"customer_correlation\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/correlation_query_\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Let them run\n",
    "time.sleep(30)\n",
    "\n",
    "# Show stream-to-static join results\n",
    "print(\"\\n\uD83D\uDCCA Stream-to-Static Join Results (Enriched Orders):\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT customer_id, product_name, brand, tier,\n",
    "           price, list_price, discount_percentage, total_value, order_category, catalog_category\n",
    "    FROM enriched_orders \n",
    "    WHERE discount_percentage > 0\n",
    "    ORDER BY discount_percentage DESC\n",
    "    LIMIT 10\n",
    "\"\"\"))\n",
    "\n",
    "# Show stream-to-stream join results\n",
    "print(\"\\n\uD83D\uDD17 Stream-to-Stream Join Results (Customer Behavior Correlation):\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT customer_id, product_name, order_category, event_type, page_category,\n",
    "           order_time, event_timestamp\n",
    "    FROM customer_correlation \n",
    "    ORDER BY order_time DESC\n",
    "    LIMIT 10\n",
    "\"\"\"))\n",
    "\n",
    "# Advanced join analytics\n",
    "print(\"\\n\uD83D\uDCC8 Advanced Join Analytics:\")\n",
    "\n",
    "# Customer purchase patterns\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_type,\n",
    "        order_category,\n",
    "        COUNT(*) as correlation_count,\n",
    "        ROUND(AVG(price), 2) as avg_order_value\n",
    "    FROM customer_correlation \n",
    "    GROUP BY event_type, order_category\n",
    "    ORDER BY correlation_count DESC\n",
    "\"\"\"))\n",
    "\n",
    "# Discount effectiveness analysis\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        tier,\n",
    "        COUNT(*) as order_count,\n",
    "        ROUND(AVG(discount_percentage), 2) as avg_discount,\n",
    "        ROUND(AVG(total_value), 2) as avg_order_value\n",
    "    FROM enriched_orders \n",
    "    WHERE discount_percentage > 0\n",
    "    GROUP BY tier\n",
    "    ORDER BY avg_order_value DESC\n",
    "\"\"\"))\n",
    "\n",
    "# Clean up\n",
    "enriched_query.stop()\n",
    "correlation_query.stop()\n",
    "\n",
    "print(\"\\n✅ Streaming joins demos complete\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Streaming Joins Best Practices:\")\n",
    "print(\"  • Use watermarks for time-based joins to manage state\")\n",
    "print(\"  • Consider join cardinality and potential data skew\")\n",
    "print(\"  • Use appropriate join types (inner vs outer)\")\n",
    "print(\"  • Monitor memory usage for stateful stream-to-stream joins\")\n",
    "print(\"  • Cache static data for stream-to-static joins\")\n",
    "print(\"  • Set time bounds for stream-to-stream joins to control state size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feac1d0b-dd36-49f0-b890-86ce8880c26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37c4576-64ef-4cb3-8d94-21baf9363339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83C\uDFED Part IV: Production Patterns\n",
    "\n",
    "## 10. Monitoring and Query Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d8c5b8-cdce-4724-bbd0-29c58903c816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Starting monitoring example...\n\n\uD83D\uDD0D Query Health Check:\nQuery Name: business_monitor\nQuery ID: 891c0222-31a5-4a53-bee0-acc8ae3e00ed\nIs Active: True\n\n\uD83D\uDCC8 Latest Progress Metrics:\n  Batch ID: 1\n  Input Rate: 0.4010909674314135 rows/sec\n  Process Rate: 0.7616146230007615 rows/sec\n  Batch Duration: 6565 ms\n  State Operations: 1\n\n\uD83D\uDCCA Current Business Metrics:\n+-----------+------------+-------------+\n|   category|total_orders|total_revenue|\n+-----------+------------+-------------+\n|Electronics|           4|      4354.12|\n|  Furniture|           2|       640.78|\n|  Furniture|           1|       288.07|\n|Electronics|           1|       208.24|\n|       Home|           2|       163.85|\n|     Sports|           2|        90.67|\n|     Sports|           1|        46.31|\n+-----------+------------+-------------+\n\n\n\uD83D\uDD04 All Active Streaming Queries:\n  1. business_monitor (ID: 891c0222-31a5-4a53-bee0-acc8ae3e00ed)\n\n✅ Monitoring example complete\n"
     ]
    }
   ],
   "source": [
    "# Create a monitoring query\n",
    "monitoring_stream = business_logic_stream \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        sum(\"price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "print(\"\uD83D\uDCCA Starting monitoring example...\")\n",
    "\n",
    "monitor_query = monitoring_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"business_monitor\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", f\"{data_dir}/streaming_checkpoints/monitor_query\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "# Let it run\n",
    "time.sleep(20)\n",
    "\n",
    "# Monitor query health\n",
    "print(\"\\n\uD83D\uDD0D Query Health Check:\")\n",
    "print(f\"Query Name: {monitor_query.name}\")\n",
    "print(f\"Query ID: {monitor_query.id}\")\n",
    "print(f\"Is Active: {monitor_query.isActive}\")\n",
    "\n",
    "# Get latest progress\n",
    "progress = monitor_query.lastProgress\n",
    "if progress:\n",
    "    print(\"\\n\uD83D\uDCC8 Latest Progress Metrics:\")\n",
    "    print(f\"  Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "    print(f\"  Input Rate: {progress.get('inputRowsPerSecond', 'N/A')} rows/sec\")\n",
    "    print(f\"  Process Rate: {progress.get('processedRowsPerSecond', 'N/A')} rows/sec\")\n",
    "    print(f\"  Batch Duration: {progress.get('batchDuration', 'N/A')} ms\")\n",
    "    \n",
    "    if 'stateOperators' in progress:\n",
    "        print(f\"  State Operations: {len(progress['stateOperators'])}\")\n",
    "\n",
    "# Show current results\n",
    "print(\"\\n\uD83D\uDCCA Current Business Metrics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT category, total_orders, ROUND(total_revenue, 2) as total_revenue\n",
    "    FROM business_monitor \n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# List all active streams\n",
    "print(\"\\n\uD83D\uDD04 All Active Streaming Queries:\")\n",
    "for i, query in enumerate(spark.streams.active, 1):\n",
    "    print(f\"  {i}. {query.name} (ID: {query.id})\")\n",
    "\n",
    "monitor_query.stop()\n",
    "print(\"\\n✅ Monitoring example complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be184217-035c-4b85-b75e-50048ce42911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\uD83D\uDEE1️ **Production Best Practices**\n",
    "===================================\n",
    "\n",
    "- **Checkpointing**: Always set checkpoint locations for fault tolerance\n",
    "- **Watermarks**: Use appropriate watermarks for windowed operations\n",
    "- **Resource Planning**: Plan cluster resources for peak loads\n",
    "- **Monitoring**: Monitor query progress and performance metrics\n",
    "- **Error Handling**: Implement proper exception handling\n",
    "- **Schema Evolution**: Plan for schema changes in data sources\n",
    "- **State Management**: Monitor state size and cleanup policies\n",
    "- **Testing**: Test with realistic data volumes and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53679f10-d2a3-4645-9823-e91e326a148b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of robust query with error handling\n",
    "print(\"\\n\uD83D\uDD27 Robust Query Example:\")\n",
    "\n",
    "try:\n",
    "    robust_query = business_logic_stream \\\n",
    "        .filter(col(\"price\") > 0) \\\n",
    "        .groupBy(\"location\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            avg(\"price\").alias(\"avg_price\")\n",
    "        ) \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"robust_example\") \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/robust_checkpoint\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    print(\"✅ Robust query started with checkpointing\")\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # Check results\n",
    "    print(\"\\n\uD83D\uDCCA Robust Query Results:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT location, order_count, ROUND(avg_price, 2) as avg_price\n",
    "        FROM robust_example \n",
    "        ORDER BY avg_price DESC\n",
    "    \"\"\").show()\n",
    "    \n",
    "    robust_query.stop()\n",
    "    print(\"✅ Robust query stopped gracefully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred: {str(e)}\")\n",
    "    print(\"\uD83D\uDCA1 This is why error handling is important in production!\")\n",
    "\n",
    "print(\"\\n\uD83C\uDFAF Key Production Recommendations:\")\n",
    "recommendations = [\n",
    "    \"Set appropriate trigger intervals (not too frequent)\",\n",
    "    \"Use UPDATE mode for aggregations, APPEND for raw data\",\n",
    "    \"Monitor memory usage and state growth\",\n",
    "    \"Plan for data source failures and recovery\",\n",
    "    \"Test thoroughly with production-like data volumes\",\n",
    "    \"Document your streaming pipeline architecture\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n✅ Production best practices covered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d78ea4-686f-4d14-83ee-ca98f3b7b6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Kafka Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739d9056-3caf-404b-9c8f-f23cb42f817c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDD17 Kafka Integration Pattern\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Sample Kafka configuration\n",
    "kafka_config = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "    \"subscribe\": \"orders_topic\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.session.timeout.ms\": \"30000\",\n",
    "    \"kafka.request.timeout.ms\": \"40000\"\n",
    "}\n",
    "\n",
    "print(\"⚙️ Kafka Configuration:\")\n",
    "for key, value in kafka_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sample order schema for Kafka messages\n",
    "kafka_order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"order_time\", TimestampType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB Kafka Message Schema:\")\n",
    "for field in kafka_order_schema.fields:\n",
    "    print(f\"  • {field.name}: {field.dataType}\")\n",
    "\n",
    "# Sample Kafka streaming code (requires actual Kafka setup)\n",
    "kafka_code_template = '''\n",
    "# Reading from Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"orders_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON messages\n",
    "parsed_orders = kafka_stream \\\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), kafka_order_schema).alias(\"data\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\"data.*\", \"kafka_timestamp\")\n",
    "\n",
    "# Process and aggregate\n",
    "kafka_analytics = parsed_orders \\\n",
    "    .withWatermark(\"order_time\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"order_time\"), \"10 minutes\"),\n",
    "        col(\"location\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"price\").alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "# Write results back to Kafka or other sink\n",
    "kafka_output = kafka_analytics \\\n",
    "    .selectExpr(\"CAST(location AS STRING) AS key\", \n",
    "                \"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"analytics_results\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "print(\"\\n\uD83D\uDCBB Kafka Integration Code Template:\")\n",
    "print(kafka_code_template)\n",
    "\n",
    "print(\"\\n\uD83D\uDD17 Alternative Data Sources for Testing:\")\n",
    "alternatives = [\n",
    "    \"File streaming: Monitor directories for new files\",\n",
    "    \"Socket streaming: TCP socket for real-time data\",\n",
    "    \"Rate source: Built-in data generator for testing\",\n",
    "    \"Memory source: In-memory data for development\"\n",
    "]\n",
    "\n",
    "for i, alt in enumerate(alternatives, 1):\n",
    "    print(f\"  {i}. {alt}\")\n",
    "\n",
    "print(\"\\n✅ Kafka integration pattern explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef025b4-3f97-425b-a2fa-44440a05e873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\uD83C\uDF89 **Congratulations! Tutorial Complete!**\n",
    "========================================\n",
    "\n",
    "**Skills Acquired:**\n",
    "- ✅ Environment setup and Spark configuration\n",
    "- ✅ Data sources and schema definition\n",
    "- ✅ ReadStream and WriteStream operations\n",
    "- ✅ Basic transformations and filtering\n",
    "- ✅ Output modes (Append, Update, Complete)\n",
    "- ✅ Trigger configurations and timing\n",
    "- ✅ Aggregations and grouping operations\n",
    "- ✅ Window operations (Tumbling, Sliding)\n",
    "- ✅ Watermarks and late data handling\n",
    "- ✅ Query monitoring and management\n",
    "- ✅ Production best practices\n",
    "- ✅ Kafka integration patterns\n",
    "\n",
    "**Next Steps:**\n",
    "1. Set up a real Kafka cluster for production testing\n",
    "2. Implement comprehensive monitoring dashboards\n",
    "3. Explore Delta Lake for ACID streaming transactions\n",
    "4. Build end-to-end streaming applications\n",
    "5. Optimize performance for your specific use cases\n",
    "6. Learn advanced topics like streaming ML pipelines\n",
    "\n",
    "**Key Takeaways:**\n",
    "• Start with simple transformations, add complexity gradually  \n",
    "• Choose appropriate output modes for your use case  \n",
    "• Set realistic watermarks based on business requirements  \n",
    "• Always plan for failure scenarios and recovery  \n",
    "• Monitor performance metrics continuously  \n",
    "• Test with production-like data volumes  \n",
    "\n",
    "\uD83E\uDDF9 **Cleaning up active streams...**\n",
    "\n",
    "\uD83C\uDF1F Thank you for completing the Spark Structured Streaming Tutorial! \uD83C\uDF1F\n",
    "\n",
    "Happy Streaming! \uD83D\uDE80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3897fcc9-ce21-4520-a53e-4f5c733361f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day3_PySpark_structured_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}